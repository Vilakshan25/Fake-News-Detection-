# -*- coding: utf-8 -*-
"""Fake News Detection.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1iqWr-q2ByvYhAFR78m4BqaDzx9Fqu-I6
"""

import nltk
nltk.download('punkt')

import matplotlib.pyplot as plt

import pandas as pd
import numpy as np
import plotly.express as pl

from google.colab import drive
drive.mount('/content/gdrive')
root_path = 'gdrive/My Drive/'

dataset=pd.read_csv('gdrive/My Drive/news.csv')

dataset

dataset.head()

dataset.tail()

dataset.isnull().sum()

fig = pl.histogram(dataset, x='label', title='Count Plot of Categories')
fig.show()

nltk.download('stopwords')
from nltk.corpus import stopwords
stopword = set(stopwords.words('english'))
from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator
text_real = " ".join(dataset[dataset['label'] == 'REAL']['text'].astype(str))
text_fake = " ".join(dataset[dataset['label'] == 'FAKE']['text'].astype(str))

title_real = " ".join(dataset[dataset['label'] == 'REAL']['title'].astype(str))
title_fake = " ".join(dataset[dataset['label'] == 'FAKE']['title'].astype(str))

wordcloud_real_text = WordCloud(
    width = 3000, height = 2000, random_state = 1, background_color = 'black', colormap = 'rainbow',
    collocations = False, stopwords = stopword
).generate(text_real)

wordcloud_fake_text = WordCloud(
    width = 3000, height = 2000, random_state = 1, background_color = 'black', colormap = 'rainbow',
    collocations = False, stopwords = stopword
).generate(text_fake)

wordcloud_real_title = WordCloud(
    width = 3000, height = 2000, random_state = 1, background_color = 'black', colormap = 'rainbow',
    collocations = False, stopwords = stopword
).generate(title_real)

wordcloud_fake_title = WordCloud(
    width = 3000, height = 2000, random_state = 1, background_color = 'black', colormap = 'rainbow',
    collocations = False, stopwords = stopword
).generate(title_fake)

fig = plt.figure(figsize = (15, 10))
plt.subplot(2, 2, 1)
plt.imshow(wordcloud_real_text, interpolation = 'bilinear')
plt.title('Real text')
plt.axis('off')
plt.subplot(2, 2, 2)
plt.imshow(wordcloud_fake_text, interpolation = 'bilinear')
plt.title('Fake text')
plt.axis('off')
plt.subplot(2, 2, 3)
plt.imshow(wordcloud_real_title, interpolation = 'bilinear')
plt.title('Real title')
plt.axis('off')
plt.subplot(2, 2, 4)
plt.imshow(wordcloud_fake_title, interpolation = 'bilinear')
plt.title('Fake title')
plt.axis('off')
plt.show()

# Tokenize the text
def tokenize_text(text):
    tokens = nltk.tokenize.word_tokenize(text)
    return tokens

from nltk.stem import PorterStemmer
import re, string
stemmer = PorterStemmer()

def clean(text):
    text = str(text).lower()
    text = re.sub('\[.*?\]', '', text)
    text = re.sub('https?://\S+|www\.\S+', '', text)
    text = re.sub('<.*?>+', '', text)
    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)
    text = re.sub('\n', '', text)
    text = re.sub('\w*\d\w*', '', text)
    text = [word for word in text.split(' ') if word not in stopword]
    text = [stemmer.stem(word) for word in text]
    text = ' '.join(text)
    return text

dataset['cleaned_text'] = dataset['text'].apply(clean)
dataset['cleaned_title'] = dataset['title'].apply(clean)
dataset.head()

from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import CountVectorizer
# Set data and target
X = dataset['cleaned_text'] + ' ' + dataset['cleaned_title']
y = dataset['label'].map({'REAL': 0, 'FAKE': 1})
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 42)
# Vectorize
vectorizer = CountVectorizer(dtype = np.float32)
X_train_vectorized = vectorizer.fit_transform(X_train)
X_test_vectorized = vectorizer.transform(X_test)


# Apply tokenization to 'cleaned_text' and 'cleaned_title' columns
dataset['tokens'] = dataset['cleaned_text'].apply(tokenize_text) + dataset['cleaned_title'].apply(tokenize_text)
from sklearn.naive_bayes import MultinomialNB
from sklearn.metrics import accuracy_score

# Initialize the Naive Bayes classifier
naive_bayes_stemmed = MultinomialNB()

# Fit the classifier on the stemmed dataset
naive_bayes_stemmed.fit(X_train_vectorized, y_train)

# Predict on the test set from the stemmed dataset
predictions_stemmed = naive_bayes_stemmed.predict(X_test_vectorized)

# Calculate accuracy on the stemmed dataset
accuracy_stemmed = accuracy_score(y_test, predictions_stemmed)
print("Accuracy on Stemmed Dataset:", accuracy_stemmed)

nltk.download('wordnet')
from nltk.stem import WordNetLemmatizer
from nltk.corpus import stopwords
import re
import string

# Initialize the WordNetLemmatizer
lemmatizer = WordNetLemmatizer()

# Define a set of stopwords
stopword = set(stopwords.words('english'))

def clean_with_lemmatization(text):
    text = str(text).lower()
    text = re.sub('\[.*?\]', '', text)
    text = re.sub('https?://\S+|www\.\S+', '', text)
    text = re.sub('<.*?>+', '', text)
    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)
    text = re.sub('\n', '', text)
    text = re.sub('\w*\d\w*', '', text)
    words = text.split()
    words = [word for word in words if word not in stopword]
    # Lemmatize each word in the list
    words = [lemmatizer.lemmatize(word) for word in words]
    text = ' '.join(words)
    return text

# Apply the function to the 'text' column
dataset['cleaned_text_lemmatized'] = dataset['text'].apply(clean_with_lemmatization)
dataset['cleaned_title_lemmatized'] = dataset['title'].apply(clean_with_lemmatization)
dataset.head()

# Set data and target for lemmatized dataset
X_lemmatized = dataset['cleaned_text_lemmatized'] + ' ' + dataset['cleaned_title_lemmatized']
y_lemmatized = dataset['label'].map({'REAL': 0, 'FAKE': 1})
dataset['tokens'] = dataset['cleaned_text_lemmatized'].apply(tokenize_text) + dataset['cleaned_title_lemmatized'].apply(tokenize_text)
# Split the lemmatized dataset into training and test sets
X_train_lemmatized, X_test_lemmatized, y_train_lemmatized, y_test_lemmatized = train_test_split(
    X_lemmatized, y_lemmatized, test_size=0.2, random_state=42)

# Vectorize the lemmatized training and test data
vectorizer_lemmatized = CountVectorizer(dtype=np.float32)
X_train_lemmatized_vectorized = vectorizer_lemmatized.fit_transform(X_train_lemmatized)
X_test_lemmatized_vectorized = vectorizer_lemmatized.transform(X_test_lemmatized)
from sklearn.naive_bayes import MultinomialNB
from sklearn.metrics import accuracy_score

# Initialize the Naive Bayes classifier for lemmatized dataset
naive_bayes_lemmatized = MultinomialNB()

# Fit the classifier on the lemmatized dataset
naive_bayes_lemmatized.fit(X_train_lemmatized_vectorized, y_train_lemmatized)

# Predict on the test set from the lemmatized dataset
predictions_nb_lemmatized = naive_bayes_lemmatized.predict(X_test_lemmatized_vectorized)

# Calculate accuracy on the lemmatized dataset using Naive Bayes
accuracy_nb_lemmatized = accuracy_score(y_test_lemmatized, predictions_nb_lemmatized)
print("Accuracy on Lemmatized Dataset (Naive Bayes):", accuracy_nb_lemmatized)

import plotly.graph_objs as go
import plotly.express as px

# Accuracy scores obtained previously
accuracy_scores = {
    'Stemmed Dataset': accuracy_stemmed,
    'Lemmatized Dataset': accuracy_nb_lemmatized
}

# Create a DataFrame for accuracy scores
accuracy_df = pd.DataFrame({'Datasets': list(accuracy_scores.keys()), 'Accuracy': list(accuracy_scores.values())})

# Plotting the accuracy comparison using Plotly
fig = px.bar(accuracy_df, x='Datasets', y='Accuracy', text='Accuracy', color='Datasets',
             labels={'Datasets': 'Dataset Type', 'Accuracy': 'Accuracy'},
             title='Comparison of Naive Bayes Accuracies (Stemmed vs Lemmatized)')
fig.update_traces(texttemplate='%{text:.4f}', textposition='outside')
fig.update_layout(yaxis=dict(range=[0, 1]), xaxis=dict(title=''))
fig.show()